{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>Dataset</th><th>Domain</th><th>No: of Series</th><th>Min. Length</th><th>Max. Length</th><th>Competition</th><th>Multivariate</th><th>Download</th><th>Source</th><th>URL</th><th>Frequency</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='str' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='str' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='int' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='int' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='int' title='int32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='str' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='str' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='str' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='str' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='str' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='str' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>M1</td><td>Multiple</td><td>1001</td><td>15</td><td>150</td><td>Yes</td><td>No</td><td>Yearly</td><td>Makridakis et al., 1982</td><td>https://zenodo.org/record/4656193</td><td>Yearly</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>M1</td><td>Multiple</td><td>1001</td><td>15</td><td>150</td><td>Yes</td><td>No</td><td>Quarterly</td><td>Makridakis et al., 1982</td><td>https://zenodo.org/record/4656154</td><td>Quarterly</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>M1</td><td>Multiple</td><td>1001</td><td>15</td><td>150</td><td>Yes</td><td>No</td><td>Monthly</td><td>Makridakis et al., 1982</td><td>https://zenodo.org/record/4656159</td><td>Monthly</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>M3</td><td>Multiple</td><td>3003</td><td>20</td><td>144</td><td>Yes</td><td>No</td><td>Yearly</td><td>Makridakis and Hibon, 2000</td><td>https://zenodo.org/record/4656222</td><td>Yearly</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>M3</td><td>Multiple</td><td>3003</td><td>20</td><td>144</td><td>Yes</td><td>No</td><td>Quarterly</td><td>Makridakis and Hibon, 2000</td><td>https://zenodo.org/record/4656262</td><td>Quarterly</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>M3</td><td>Multiple</td><td>3003</td><td>20</td><td>144</td><td>Yes</td><td>No</td><td>Monthly</td><td>Makridakis and Hibon, 2000</td><td>https://zenodo.org/record/4656298</td><td>Monthly</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>M3</td><td>Multiple</td><td>3003</td><td>20</td><td>144</td><td>Yes</td><td>No</td><td>Other</td><td>Makridakis and Hibon, 2000</td><td>https://zenodo.org/record/4656335</td><td>Other</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>M4</td><td>Multiple</td><td>100000</td><td>19</td><td>9933</td><td>Yes</td><td>No</td><td>Yearly</td><td>Makridakis et al., 2020</td><td>https://zenodo.org/record/4656379</td><td>Yearly</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>M4</td><td>Multiple</td><td>100000</td><td>19</td><td>9933</td><td>Yes</td><td>No</td><td>Quarterly</td><td>Makridakis et al., 2020</td><td>https://zenodo.org/record/4656410</td><td>Quarterly</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>M4</td><td>Multiple</td><td>100000</td><td>19</td><td>9933</td><td>Yes</td><td>No</td><td>Monthly</td><td>Makridakis et al., 2020</td><td>https://zenodo.org/record/4656480</td><td>Monthly</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>10 rows &times; 11 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<Frame#7fc8d003c0f0 10x11>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import datatable as dt \n",
    "import pandas as pd \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "\n",
    "df = dt.fread(\"monash-repository.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://zenodo.org/record/4656193'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df['URL'][0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Soup\n",
    "page = requests.get(x)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/record/4656193/files/m1_yearly_dataset.zip?download=1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = soup.select(\"[class~=filename]\")\n",
    "[i.get('href') for i in info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/record/4656193/files/m1_yearly_dataset.zip?download=1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = info[0]\n",
    "check.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('check.zip', <http.client.HTTPMessage at 0x7fc8d0026e20>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Download File\n",
    "urllib.request.urlretrieve(\"https://zenodo.org\" + check.get('href'), \"check.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "# full_file_path_and_name - complete .tsf file path\n",
    "# replace_missing_vals_with - a term to indicate the missing values in series in the returning dataframe\n",
    "# value_column_name - Any name that is preferred to have as the name of the column containing series values in the returning dataframe\n",
    "def convert_tsf_to_dataframe(\n",
    "    full_file_path_and_name,\n",
    "    replace_missing_vals_with=\"NaN\",\n",
    "    value_column_name=\"series_value\",\n",
    "):\n",
    "    '''Converts the contents in a .tsf file into a dataframe and returns it along with other meta-data of the dataset: frequency, horizon, whether the dataset contains missing values and whether the series have equal lengths'''\n",
    "    col_names = []\n",
    "    col_types = []\n",
    "    all_data = {}\n",
    "    line_count = 0\n",
    "    frequency = None\n",
    "    forecast_horizon = None\n",
    "    contain_missing_values = None\n",
    "    contain_equal_length = None\n",
    "    found_data_tag = False\n",
    "    found_data_section = False\n",
    "    started_reading_data_section = False\n",
    "\n",
    "    with open(full_file_path_and_name, \"r\", encoding=\"cp1252\") as file:\n",
    "        for line in file:\n",
    "            # Strip white space from start/end of line\n",
    "            line = line.strip()\n",
    "\n",
    "            if line:\n",
    "                if line.startswith(\"@\"):  # Read meta-data\n",
    "                    if not line.startswith(\"@data\"):\n",
    "                        line_content = line.split(\" \")\n",
    "                        if line.startswith(\"@attribute\"):\n",
    "                            if (\n",
    "                                len(line_content) != 3\n",
    "                            ):  # Attributes have both name and type\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            col_names.append(line_content[1])\n",
    "                            col_types.append(line_content[2])\n",
    "                        else:\n",
    "                            if (\n",
    "                                len(line_content) != 2\n",
    "                            ):  # Other meta-data have only values\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            if line.startswith(\"@frequency\"):\n",
    "                                frequency = line_content[1]\n",
    "                            elif line.startswith(\"@horizon\"):\n",
    "                                forecast_horizon = int(line_content[1])\n",
    "                            elif line.startswith(\"@missing\"):\n",
    "                                contain_missing_values = bool(\n",
    "                                    strtobool(line_content[1])\n",
    "                                )\n",
    "                            elif line.startswith(\"@equallength\"):\n",
    "                                contain_equal_length = bool(strtobool(line_content[1]))\n",
    "\n",
    "                    else:\n",
    "                        if len(col_names) == 0:\n",
    "                            raise Exception(\n",
    "                                \"Missing attribute section. Attribute section must come before data.\"\n",
    "                            )\n",
    "\n",
    "                        found_data_tag = True\n",
    "                elif not line.startswith(\"#\"):\n",
    "                    if len(col_names) == 0:\n",
    "                        raise Exception(\n",
    "                            \"Missing attribute section. Attribute section must come before data.\"\n",
    "                        )\n",
    "                    elif not found_data_tag:\n",
    "                        raise Exception(\"Missing @data tag.\")\n",
    "                    else:\n",
    "                        if not started_reading_data_section:\n",
    "                            started_reading_data_section = True\n",
    "                            found_data_section = True\n",
    "                            all_series = []\n",
    "\n",
    "                            for col in col_names:\n",
    "                                all_data[col] = []\n",
    "\n",
    "                        full_info = line.split(\":\")\n",
    "\n",
    "                        if len(full_info) != (len(col_names) + 1):\n",
    "                            raise Exception(\"Missing attributes/values in series.\")\n",
    "\n",
    "                        series = full_info[len(full_info) - 1]\n",
    "                        series = series.split(\",\")\n",
    "\n",
    "                        if len(series) == 0:\n",
    "                            raise Exception(\n",
    "                                \"A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series. Missing values should be indicated with ? symbol\"\n",
    "                            )\n",
    "\n",
    "                        numeric_series = []\n",
    "\n",
    "                        for val in series:\n",
    "                            if val == \"?\":\n",
    "                                numeric_series.append(replace_missing_vals_with)\n",
    "                            else:\n",
    "                                numeric_series.append(float(val))\n",
    "\n",
    "                        if numeric_series.count(replace_missing_vals_with) == len(\n",
    "                            numeric_series\n",
    "                        ):\n",
    "                            raise Exception(\n",
    "                                \"All series values are missing. A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series.\"\n",
    "                            )\n",
    "\n",
    "                        all_series.append(pd.Series(numeric_series).array)\n",
    "\n",
    "                        for i in range(len(col_names)):\n",
    "                            att_val = None\n",
    "                            if col_types[i] == \"numeric\":\n",
    "                                att_val = int(full_info[i])\n",
    "                            elif col_types[i] == \"string\":\n",
    "                                att_val = str(full_info[i])\n",
    "                            elif col_types[i] == \"date\":\n",
    "                                att_val = datetime.strptime(\n",
    "                                    full_info[i], \"%Y-%m-%d %H-%M-%S\"\n",
    "                                )\n",
    "                            else:\n",
    "                                raise Exception(\n",
    "                                    \"Invalid attribute type.\"\n",
    "                                )  # Currently, the code supports only numeric, string and date types. Extend this as required.\n",
    "\n",
    "                            if att_val is None:\n",
    "                                raise Exception(\"Invalid attribute value.\")\n",
    "                            else:\n",
    "                                all_data[col_names[i]].append(att_val)\n",
    "\n",
    "                line_count = line_count + 1\n",
    "\n",
    "        if line_count == 0:\n",
    "            raise Exception(\"Empty file.\")\n",
    "        if len(col_names) == 0:\n",
    "            raise Exception(\"Missing attribute section.\")\n",
    "        if not found_data_section:\n",
    "            raise Exception(\"Missing series information under data section.\")\n",
    "\n",
    "        all_data[value_column_name] = all_series\n",
    "        loaded_data = pd.DataFrame(all_data)\n",
    "\n",
    "        return (\n",
    "            loaded_data,\n",
    "            frequency,\n",
    "            forecast_horizon,\n",
    "            contain_missing_values,\n",
    "            contain_equal_length,\n",
    "        )\n",
    "\n",
    "def parse_monash_df(file):\n",
    "    \n",
    "    ''' Function to Parse a Locally Extracted and Downloaded File'''\n",
    "    \n",
    "    loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(file)\n",
    "\n",
    "    parsed_df = pd.DataFrame()\n",
    "\n",
    "    #freq = frequency\n",
    "    if frequency == 'yearly':\n",
    "        freq = 'YS' #year start\n",
    "    elif frequency == 'quarterly':\n",
    "        freq = 'QS' #quarter start\n",
    "    elif frequency == 'monthly':\n",
    "        freq = 'MS'\n",
    "    elif frequency == 'daily':\n",
    "        freq = 'D'\n",
    "\n",
    "\n",
    "    for index,row in tqdm(loaded_data.iterrows()):\n",
    "        \n",
    "        name = row.series_name\n",
    "        #print(name)\n",
    "        values = row.series_value.tolist()\n",
    "        length = len(values)\n",
    "        start = row.start_timestamp\n",
    "        #print(length)\n",
    "        ds = pd.date_range(start, periods=length, freq=freq)\n",
    "        \n",
    "        series_df = pd.DataFrame({'unique_id':name,'ds':ds, 'values':values})\n",
    "        parsed_df = pd.concat([parsed_df, series_df], axis=0)\n",
    "        \n",
    "    return parsed_df\n",
    "\n",
    "# Example Usage\n",
    "#parse_monash_df(\"m1_yearly_dataset.tsf\").head()\n",
    "\n",
    "def extract_monash_df(archive_file, origindestination):\n",
    "    ''' Function to unzip and extract downloaded archive file'''   \n",
    "\n",
    "    os.chdir(destination)\n",
    "\n",
    "    dest = 'staging'\n",
    "    print(dest)\n",
    "    orig = os.getcwd()\n",
    "    print(orig)\n",
    "\n",
    "    # Create Staging Directory to Extract Archive File\n",
    "    os.makedirs(dest, exist_ok=True)\n",
    "    \n",
    "    # Copy Archive File into Staging Directory\n",
    "    shutil.copy(archive_file, dest)\n",
    "    \n",
    "    # Go into the directory and unpack\n",
    "    os.chdir(dest)\n",
    "    shutil.unpack_archive(archive_file\n",
    "                          )\n",
    "    # Find any TSF Files\n",
    "    result = glob.glob('*.{}'.format('tsf'))\n",
    "    #print(result[0])\n",
    "    \n",
    "    # Find the name of the file \n",
    "    local_name = result[0].split(\".\")[0]\n",
    "    \n",
    "    # Parse the DataFrame\n",
    "    local_df = parse_monash_df(result[0])\n",
    "    \n",
    "    os.chdir(destination)\n",
    "    try:\n",
    "        shutil.rmtree(dest)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s : %s\" % (dest, e.strerror))\n",
    "        \n",
    "    return local_name, local_df\n",
    "\n",
    "# Example Usage\n",
    "#check = extract_monash_df('tmp.zip')\n",
    "\n",
    "def retrieve_monash_df(url, destination):\n",
    "    \n",
    "    # Create Soup\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    # Create Soup\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # Find Download URL from Filename Class\n",
    "    info = soup.select(\"[class~=filename]\")\n",
    "    \n",
    "    # Parse Download Url\n",
    "    download_url = info[0].get('href')\n",
    "    \n",
    "    # Download File\n",
    "    urllib.request.urlretrieve(\"https://zenodo.org\" + download_url, \"tmp.zip\")\n",
    "    \n",
    "# Example Usage\n",
    "#retrieve_monash_df(df['URL'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'datatable.Frame' object has no attribute 'isin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/01/4mysj8cx1bjg_w8rbw097jwr0000gp/T/ipykernel_78270/2828861709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Focus on just a few datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Frequency'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Daily'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Monthly'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Yearly'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'datatable.Frame' object has no attribute 'isin'"
     ]
    }
   ],
   "source": [
    "# Focus on just a few datasets\n",
    "df = df[df['Frequency'].isin(['Daily', 'Monthly', 'Yearly'])]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jfarland/Documents/research/timeseries-benchmarks'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path = 'Users/jfarland/Documents/research/timeseries-benchmarks'\n",
    "data_path = project_path + '/data'\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRIEVING URL: https://zenodo.org/record/4656548\n",
      "PARSING AND EXTRACTING\n",
      "staging\n",
      "/Users/jfarland/Documents/products/tidal-pulse/data\n",
      "m4_daily_dataset.tsf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4227it [05:05, 13.85it/s]\n",
      "1it [07:01, 421.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRIEVING URL: https://zenodo.org/record/4654822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [08:48, 528.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/01/4mysj8cx1bjg_w8rbw097jwr0000gp/T/ipykernel_86559/638856603.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'RETRIEVING URL: {url}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mretrieve_monash_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'PARSING AND EXTRACTING'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/01/4mysj8cx1bjg_w8rbw097jwr0000gp/T/ipykernel_86559/84481937.py\u001b[0m in \u001b[0;36mretrieve_monash_df\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Download File\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://zenodo.org\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdownload_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tmp.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mretrieve_monash_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'URL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tidal-pulse/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tidal-pulse/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tidal-pulse/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tidal-pulse/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tidal-pulse/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tidal-pulse/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "from datetime import datetime\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    \n",
    "    # Make sure current directory is data directory\n",
    "    os.chdir(data_path)\n",
    "    \n",
    "    # Identify url from mapping file\n",
    "    url = row.URL\n",
    "    print(f'RETRIEVING URL: {url}')\n",
    "    \n",
    "    # Retrieve the file from the url\n",
    "    retrieve_monash_df(url)\n",
    "    \n",
    "    print(f'PARSING AND EXTRACTING')\n",
    "    local_name, local_df = extract_monash_df('tmp.zip')\n",
    "    \n",
    "    # create subdirectory\n",
    "    os.makedirs(local_name)\n",
    "    local_df.to_csv(local_name + '/' + local_name + '.csv', index=False)\n",
    "    \n",
    "    \n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Converts the contents in a .tsf file into a dataframe and returns it along with other meta-data of the dataset: frequency, horizon, whether the dataset contains missing values and whether the series have equal lengths\n",
    "#\n",
    "# Parameters\n",
    "# full_file_path_and_name - complete .tsf file path\n",
    "# replace_missing_vals_with - a term to indicate the missing values in series in the returning dataframe\n",
    "# value_column_name - Any name that is preferred to have as the name of the column containing series values in the returning dataframe\n",
    "def convert_tsf_to_dataframe(\n",
    "    full_file_path_and_name,\n",
    "    replace_missing_vals_with=\"NaN\",\n",
    "    value_column_name=\"series_value\",\n",
    "):\n",
    "    col_names = []\n",
    "    col_types = []\n",
    "    all_data = {}\n",
    "    line_count = 0\n",
    "    frequency = None\n",
    "    forecast_horizon = None\n",
    "    contain_missing_values = None\n",
    "    contain_equal_length = None\n",
    "    found_data_tag = False\n",
    "    found_data_section = False\n",
    "    started_reading_data_section = False\n",
    "\n",
    "    with open(full_file_path_and_name, \"r\", encoding=\"cp1252\") as file:\n",
    "        for line in file:\n",
    "            # Strip white space from start/end of line\n",
    "            line = line.strip()\n",
    "\n",
    "            if line:\n",
    "                if line.startswith(\"@\"):  # Read meta-data\n",
    "                    if not line.startswith(\"@data\"):\n",
    "                        line_content = line.split(\" \")\n",
    "                        if line.startswith(\"@attribute\"):\n",
    "                            if (\n",
    "                                len(line_content) != 3\n",
    "                            ):  # Attributes have both name and type\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            col_names.append(line_content[1])\n",
    "                            col_types.append(line_content[2])\n",
    "                        else:\n",
    "                            if (\n",
    "                                len(line_content) != 2\n",
    "                            ):  # Other meta-data have only values\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            if line.startswith(\"@frequency\"):\n",
    "                                frequency = line_content[1]\n",
    "                            elif line.startswith(\"@horizon\"):\n",
    "                                forecast_horizon = int(line_content[1])\n",
    "                            elif line.startswith(\"@missing\"):\n",
    "                                contain_missing_values = bool(\n",
    "                                    strtobool(line_content[1])\n",
    "                                )\n",
    "                            elif line.startswith(\"@equallength\"):\n",
    "                                contain_equal_length = bool(strtobool(line_content[1]))\n",
    "\n",
    "                    else:\n",
    "                        if len(col_names) == 0:\n",
    "                            raise Exception(\n",
    "                                \"Missing attribute section. Attribute section must come before data.\"\n",
    "                            )\n",
    "\n",
    "                        found_data_tag = True\n",
    "                elif not line.startswith(\"#\"):\n",
    "                    if len(col_names) == 0:\n",
    "                        raise Exception(\n",
    "                            \"Missing attribute section. Attribute section must come before data.\"\n",
    "                        )\n",
    "                    elif not found_data_tag:\n",
    "                        raise Exception(\"Missing @data tag.\")\n",
    "                    else:\n",
    "                        if not started_reading_data_section:\n",
    "                            started_reading_data_section = True\n",
    "                            found_data_section = True\n",
    "                            all_series = []\n",
    "\n",
    "                            for col in col_names:\n",
    "                                all_data[col] = []\n",
    "\n",
    "                        full_info = line.split(\":\")\n",
    "\n",
    "                        if len(full_info) != (len(col_names) + 1):\n",
    "                            raise Exception(\"Missing attributes/values in series.\")\n",
    "\n",
    "                        series = full_info[len(full_info) - 1]\n",
    "                        series = series.split(\",\")\n",
    "\n",
    "                        if len(series) == 0:\n",
    "                            raise Exception(\n",
    "                                \"A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series. Missing values should be indicated with ? symbol\"\n",
    "                            )\n",
    "\n",
    "                        numeric_series = []\n",
    "\n",
    "                        for val in series:\n",
    "                            if val == \"?\":\n",
    "                                numeric_series.append(replace_missing_vals_with)\n",
    "                            else:\n",
    "                                numeric_series.append(float(val))\n",
    "\n",
    "                        if numeric_series.count(replace_missing_vals_with) == len(\n",
    "                            numeric_series\n",
    "                        ):\n",
    "                            raise Exception(\n",
    "                                \"All series values are missing. A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series.\"\n",
    "                            )\n",
    "\n",
    "                        all_series.append(pd.Series(numeric_series).array)\n",
    "\n",
    "                        for i in range(len(col_names)):\n",
    "                            att_val = None\n",
    "                            if col_types[i] == \"numeric\":\n",
    "                                att_val = int(full_info[i])\n",
    "                            elif col_types[i] == \"string\":\n",
    "                                att_val = str(full_info[i])\n",
    "                            elif col_types[i] == \"date\":\n",
    "                                att_val = datetime.strptime(\n",
    "                                    full_info[i], \"%Y-%m-%d %H-%M-%S\"\n",
    "                                )\n",
    "                            else:\n",
    "                                raise Exception(\n",
    "                                    \"Invalid attribute type.\"\n",
    "                                )  # Currently, the code supports only numeric, string and date types. Extend this as required.\n",
    "\n",
    "                            if att_val is None:\n",
    "                                raise Exception(\"Invalid attribute value.\")\n",
    "                            else:\n",
    "                                all_data[col_names[i]].append(att_val)\n",
    "\n",
    "                line_count = line_count + 1\n",
    "\n",
    "        if line_count == 0:\n",
    "            raise Exception(\"Empty file.\")\n",
    "        if len(col_names) == 0:\n",
    "            raise Exception(\"Missing attribute section.\")\n",
    "        if not found_data_section:\n",
    "            raise Exception(\"Missing series information under data section.\")\n",
    "\n",
    "        all_data[value_column_name] = all_series\n",
    "        loaded_data = pd.DataFrame(all_data)\n",
    "\n",
    "        return (\n",
    "            loaded_data,\n",
    "            frequency,\n",
    "            forecast_horizon,\n",
    "            contain_missing_values,\n",
    "            contain_equal_length,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    series_name start_timestamp  \\\n",
      "0            T1      1972-01-01   \n",
      "1            T2      1974-01-01   \n",
      "2            T3      1974-01-01   \n",
      "3            T4      1974-01-01   \n",
      "4            T5      1976-01-01   \n",
      "..          ...             ...   \n",
      "176        T177      1974-01-01   \n",
      "177        T178      1973-01-01   \n",
      "178        T179      1973-01-01   \n",
      "179        T180      1975-01-01   \n",
      "180        T181      1975-01-01   \n",
      "\n",
      "                                          series_value  \n",
      "0    [3600.0, 7700.0, 12300.0, 30500.0, 47390.0, 57...  \n",
      "1    [12654.0, 22879.0, 34164.0, 49524.0, 64761.0, ...  \n",
      "2    [2142.0, 12935.0, 19130.0, 30500.0, 48177.0, 5...  \n",
      "3    [5774.0, 7650.0, 9271.0, 21447.0, 28998.0, 409...  \n",
      "4    [432312.0, 569011.0, 862673.0, 1155640.0, 1439...  \n",
      "..                                                 ...  \n",
      "176  [290783.0, 285242.0, 293718.0, 295804.0, 29458...  \n",
      "177  [11693.0, 11702.0, 11703.0, 11557.0, 11951.0, ...  \n",
      "178  [8438.0, 8689.0, 8590.0, 8763.0, 8710.0, 8837....  \n",
      "179  [55.91, 54.7, 55.3, 55.75, 55.46, 55.37, 53.82...  \n",
      "180  [564.0, 841.0, 854.0, 689.0, 559.0, 510.0, 997...  \n",
      "\n",
      "[181 rows x 3 columns]\n",
      "yearly\n",
      "6\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Example of usage\n",
    "loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"m1_yearly_dataset.tsf\")\n",
    "\n",
    "print(loaded_data)\n",
    "print(frequency)\n",
    "print(forecast_horizon)\n",
    "print(contain_missing_values)\n",
    "print(contain_equal_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_name</th>\n",
       "      <th>start_timestamp</th>\n",
       "      <th>series_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>1972-01-01</td>\n",
       "      <td>[3600.0, 7700.0, 12300.0, 30500.0, 47390.0, 57...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>1974-01-01</td>\n",
       "      <td>[12654.0, 22879.0, 34164.0, 49524.0, 64761.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>1974-01-01</td>\n",
       "      <td>[2142.0, 12935.0, 19130.0, 30500.0, 48177.0, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>1974-01-01</td>\n",
       "      <td>[5774.0, 7650.0, 9271.0, 21447.0, 28998.0, 409...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5</td>\n",
       "      <td>1976-01-01</td>\n",
       "      <td>[432312.0, 569011.0, 862673.0, 1155640.0, 1439...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  series_name start_timestamp  \\\n",
       "0          T1      1972-01-01   \n",
       "1          T2      1974-01-01   \n",
       "2          T3      1974-01-01   \n",
       "3          T4      1974-01-01   \n",
       "4          T5      1976-01-01   \n",
       "\n",
       "                                        series_value  \n",
       "0  [3600.0, 7700.0, 12300.0, 30500.0, 47390.0, 57...  \n",
       "1  [12654.0, 22879.0, 34164.0, 49524.0, 64761.0, ...  \n",
       "2  [2142.0, 12935.0, 19130.0, 30500.0, 48177.0, 5...  \n",
       "3  [5774.0, 7650.0, 9271.0, 21447.0, 28998.0, 409...  \n",
       "4  [432312.0, 569011.0, 862673.0, 1155640.0, 1439...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "check_is_scitype() missing 1 required positional argument: 'scitype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/01/4mysj8cx1bjg_w8rbw097jwr0000gp/T/ipykernel_86559/1048451860.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msktime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatatypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_is_scitype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcheck_is_scitype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: check_is_scitype() missing 1 required positional argument: 'scitype'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#import sktime\n",
    "#from sktime.utils.data_io import load_from_tsfile_to_dataframe\n",
    "from sktime.datatypes import check_is_scitype\n",
    "\n",
    "check_is_scitype(loaded_data)\n",
    "\n",
    "\n",
    "\n",
    "#DATA_PATH = os.path.join(os.path.dirname(sktime.__file__), \"datasets/data\")\n",
    "\n",
    "# train_x, train_y = load_from_tsfile_to_dataframe(\n",
    "#     os.path.join(DATA_PATH, \"ArrowHead/ArrowHead_TRAIN.ts\")\n",
    "# )\n",
    "# test_x, test_y = load_from_tsfile_to_dataframe(\n",
    "#     os.path.join(DATA_PATH, \"ArrowHead/ArrowHead_TEST.ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_name</th>\n",
       "      <th>start_timestamp</th>\n",
       "      <th>series_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>1972-01-01</td>\n",
       "      <td>[3600.0, 7700.0, 12300.0, 30500.0, 47390.0, 57...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>1974-01-01</td>\n",
       "      <td>[12654.0, 22879.0, 34164.0, 49524.0, 64761.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>1974-01-01</td>\n",
       "      <td>[2142.0, 12935.0, 19130.0, 30500.0, 48177.0, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>1974-01-01</td>\n",
       "      <td>[5774.0, 7650.0, 9271.0, 21447.0, 28998.0, 409...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5</td>\n",
       "      <td>1976-01-01</td>\n",
       "      <td>[432312.0, 569011.0, 862673.0, 1155640.0, 1439...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>T177</td>\n",
       "      <td>1974-01-01</td>\n",
       "      <td>[290783.0, 285242.0, 293718.0, 295804.0, 29458...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>T178</td>\n",
       "      <td>1973-01-01</td>\n",
       "      <td>[11693.0, 11702.0, 11703.0, 11557.0, 11951.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>T179</td>\n",
       "      <td>1973-01-01</td>\n",
       "      <td>[8438.0, 8689.0, 8590.0, 8763.0, 8710.0, 8837....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>T180</td>\n",
       "      <td>1975-01-01</td>\n",
       "      <td>[55.91, 54.7, 55.3, 55.75, 55.46, 55.37, 53.82...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>T181</td>\n",
       "      <td>1975-01-01</td>\n",
       "      <td>[564.0, 841.0, 854.0, 689.0, 559.0, 510.0, 997...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    series_name start_timestamp  \\\n",
       "0            T1      1972-01-01   \n",
       "1            T2      1974-01-01   \n",
       "2            T3      1974-01-01   \n",
       "3            T4      1974-01-01   \n",
       "4            T5      1976-01-01   \n",
       "..          ...             ...   \n",
       "176        T177      1974-01-01   \n",
       "177        T178      1973-01-01   \n",
       "178        T179      1973-01-01   \n",
       "179        T180      1975-01-01   \n",
       "180        T181      1975-01-01   \n",
       "\n",
       "                                          series_value  \n",
       "0    [3600.0, 7700.0, 12300.0, 30500.0, 47390.0, 57...  \n",
       "1    [12654.0, 22879.0, 34164.0, 49524.0, 64761.0, ...  \n",
       "2    [2142.0, 12935.0, 19130.0, 30500.0, 48177.0, 5...  \n",
       "3    [5774.0, 7650.0, 9271.0, 21447.0, 28998.0, 409...  \n",
       "4    [432312.0, 569011.0, 862673.0, 1155640.0, 1439...  \n",
       "..                                                 ...  \n",
       "176  [290783.0, 285242.0, 293718.0, 295804.0, 29458...  \n",
       "177  [11693.0, 11702.0, 11703.0, 11557.0, 11951.0, ...  \n",
       "178  [8438.0, 8689.0, 8590.0, 8763.0, 8710.0, 8837....  \n",
       "179  [55.91, 54.7, 55.3, 55.75, 55.46, 55.37, 53.82...  \n",
       "180  [564.0, 841.0, 854.0, 689.0, 559.0, 510.0, 997...  \n",
       "\n",
       "[181 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yearly'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_name</th>\n",
       "      <th>start_timestamp</th>\n",
       "      <th>series_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>1972-01-01</td>\n",
       "      <td>[3600.0, 7700.0, 12300.0, 30500.0, 47390.0, 57...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  series_name start_timestamp  \\\n",
       "0          T1      1972-01-01   \n",
       "\n",
       "                                        series_value  \n",
       "0  [3600.0, 7700.0, 12300.0, 30500.0, 47390.0, 57...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = loaded_data.head(1)\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "181it [00:00, 1246.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>1972-01-01</td>\n",
       "      <td>3600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1</td>\n",
       "      <td>1973-01-01</td>\n",
       "      <td>7700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1</td>\n",
       "      <td>1974-01-01</td>\n",
       "      <td>12300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T1</td>\n",
       "      <td>1975-01-01</td>\n",
       "      <td>30500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T1</td>\n",
       "      <td>1976-01-01</td>\n",
       "      <td>47390.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id         ds   values\n",
       "0        T1 1972-01-01   3600.0\n",
       "1        T1 1973-01-01   7700.0\n",
       "2        T1 1974-01-01  12300.0\n",
       "3        T1 1975-01-01  30500.0\n",
       "4        T1 1976-01-01  47390.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "parsed_df = pd.DataFrame()\n",
    "\n",
    "#freq = frequency\n",
    "if frequency == 'yearly':\n",
    "    freq = 'YS' #year start\n",
    "elif frequency == 'quarterly':\n",
    "    freq = 'QS' #quarter start\n",
    "elif frequency == 'monthly':\n",
    "    freq = 'MS'\n",
    "\n",
    "\n",
    "for index,row in tqdm(loaded_data.iterrows()):\n",
    "    \n",
    "    name = row.series_name\n",
    "    #print(name)\n",
    "    values = row.series_value.tolist()\n",
    "    length = len(values)\n",
    "    start = row.start_timestamp\n",
    "    #print(length)\n",
    "    ds = pd.date_range(start, periods=length, freq=freq)\n",
    "    \n",
    "    series_df = pd.DataFrame({'unique_id':name,'ds':ds, 'values':values})\n",
    "    parsed_df = pd.concat([parsed_df, series_df], axis=0)\n",
    "    \n",
    "parsed_df.head()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_monash_df(file):\n",
    "    \n",
    "    loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(file)\n",
    "\n",
    "    parsed_df = pd.DataFrame()\n",
    "\n",
    "    #freq = frequency\n",
    "    if frequency == 'yearly':\n",
    "        freq = 'YS' #year start\n",
    "    elif frequency == 'quarterly':\n",
    "        freq = 'QS' #quarter start\n",
    "    elif frequency == 'monthly':\n",
    "        freq = 'MS'\n",
    "    elif frequency == 'daily':\n",
    "        freq = 'D'\n",
    "\n",
    "\n",
    "    for index,row in tqdm(loaded_data.iterrows()):\n",
    "        \n",
    "        name = row.series_name\n",
    "        #print(name)\n",
    "        values = row.series_value.tolist()\n",
    "        length = len(values)\n",
    "        start = row.start_timestamp\n",
    "        #print(length)\n",
    "        ds = pd.date_range(start, periods=length, freq=freq)\n",
    "        \n",
    "        series_df = pd.DataFrame({'unique_id':name,'ds':ds, 'values':values})\n",
    "        parsed_df = pd.concat([parsed_df, series_df], axis=0)\n",
    "        \n",
    "    return parsed_df\n",
    "\n",
    "#parse_monash_df(\"m1_yearly_dataset.tsf\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tidal-pulse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "914ceb58c6ac842b05884c1be4a2ad0b416b6dbedaf25f0bde18e10f251005c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
